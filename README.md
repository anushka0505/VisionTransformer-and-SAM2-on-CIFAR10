# VisionTransformer-and-SAM2-on-CIFAR10
# ğŸ¯ Vision Transformer on CIFAR-10
Assignment: Q1 â€” Vision Transformer on CIFAR-10 (PyTorch)
Implementation of a Vision Transformer (ViT) for CIFAR-10 image classification, based on the paper "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" (Dosovitskiy et al., ICLR 2021).
```python
Overview
This project implements a compact Vision Transformer trained on CIFAR-10 with various regularization techniques and training tricks to maximize test accuracy.

Architecture Components
Patch Embedding: Convolutional layer to split images into patches and project to embedding dimension
CLS Token: Learnable classification token prepended to sequence
Positional Embeddings: Learnable position encodings for spatial information
Transformer Encoder: Stack of Multi-Head Self-Attention (MHSA) + MLP blocks with residual connections and layer normalization
Classification Head: Linear layer that classifies from the CLS token representation

How to Run in Google Colab
Step 1: Setup Environment
python   # No additional installations needed - uses standard PyTorch
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

Step 2: Upload and Run Code
Open Google Colab
Upload the vit_cifar10.py file or copy-paste the code into a cell
Run the cell (the dataset will auto-download on first run)

Step 3: Monitor Training
The script will:
Download CIFAR-10 dataset automatically
Train the model with progress output each epoch
Apply early stopping when validation accuracy plateaus
Display a training/test accuracy curve at the end
Expected Runtime: ~15-25 minutes on Colab GPU (T4)

Best Model Configuration
HyperparameterValueDescriptionPatch Size4Ã—4Divides 32Ã—32 images into 8Ã—8 = 64 patchesEmbedding Dim128Feature dimension for tokensDepth6Number of Transformer encoder layersAttention Heads4Multi-head attention headsMLP Ratio2.0Hidden dimension multiplier for MLP blocksDropout0.1Dropout rateBatch Size128Training batch sizeOptimizerAdamWWith weight decay 0.05Learning Rate3e-4Base learning rateLR ScheduleWarmup (5 ep) + CosineWarmup then cosine annealingEpochs120Maximum epochs (with early stopping)Label Smoothing0.1Smoothing for cross-entropy lossEMA0.999Exponential Moving Average of weightsMixupÎ±=0.2Data augmentation mixing parameter

Data Augmentation
Random resized crop (scale 0.8-1.0)
Random horizontal flip
Random erasing (p=0.25)
Mixup (Î±=0.2)

Results
MetricValueTest Accuracy~82-85%Training Time~15-25 min (Colab T4 GPU)Parameters~0.5MEarly StoppingPatience = 5 epochs
Performance Notes

EMA model is used for evaluation (provides more stable predictions)
Early stopping prevents overfitting
Best accuracy typically achieved around epochs 60-100
Results may vary slightly due to random initialization and augmentation

Key Implementation Details

1. Vision Transformer Architecture
pythonInput Image (3Ã—32Ã—32)
    â†“ Conv2d patch embedding (4Ã—4 patches)
Patch Tokens (64Ã—128)
    â†“ Prepend CLS token + Add positional embeddings
Token Sequence (65Ã—128)
    â†“ 6Ã— Transformer Encoder Layers
        - Multi-Head Self-Attention (4 heads)
        - MLP (128â†’256â†’128)
        - Layer Norm (pre-norm)
        - Residual connections
Encoded Sequence (65Ã—128)
    â†“ Extract CLS token
CLS Representation (128)
    â†“ Linear classification head
Output Logits (10 classes)

2. Training Techniques
Mixup: Mixes pairs of images and labels for better generalization
EMA: Maintains exponential moving average of model weights
Label Smoothing: Reduces overconfidence in predictions
Warmup + Cosine LR: Gradual warmup followed by cosine decay
Early Stopping: Stops when validation accuracy doesn't improve

3. Regularization
Weight decay (0.05)
Dropout (0.1)
Random erasing augmentation
Label smoothing (0.1)

Code Structure
â”œâ”€â”€ Dataset Loading (CIFAR-10 with augmentation)
â”œâ”€â”€ ViT Model Definition
â”‚   â”œâ”€â”€ Patch Embedding (Conv2d)
â”‚   â”œâ”€â”€ CLS Token & Positional Embedding
â”‚   â”œâ”€â”€ Transformer Encoder Stack
â”‚   â””â”€â”€ Classification Head
â”œâ”€â”€ Training Utilities
â”‚   â”œâ”€â”€ Mixup augmentation
â”‚   â”œâ”€â”€ EMA weight update
â”‚   â””â”€â”€ Warmup-Cosine LR scheduler
â”œâ”€â”€ Training Loop
â”‚   â”œâ”€â”€ Train one epoch
â”‚   â”œâ”€â”€ Evaluate on test set
â”‚   â””â”€â”€ Early stopping logic
â””â”€â”€ Visualization (Accuracy curves)

Dependencies
All dependencies are pre-installed in Google Colab:
PyTorch >= 1.10
torchvision
numpy
matplotlib
tqdm

References
Paper: Dosovitskiy, A., et al. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021.
Dataset: CIFAR-10 (Krizhevsky, 2009)

ğŸ”® Tips for Further Improvement
Increase model capacity: Try larger embedding dimensions (192, 256) or more layers
Data augmentation: Add AutoAugment, CutMix, or RandAugment
Training longer: Increase epochs to 200-300 with adjusted patience
Stochastic depth: Add drop path regularization
Knowledge distillation: Use a teacher model for soft labels
Test-time augmentation: Average predictions over multiple augmented views


Note: This is a lightweight ViT designed for CIFAR-10's small 32Ã—32 images. For larger datasets like ImageNet, you would typically use larger patch sizes (16Ã—16) and deeper models.
```

# ğŸ¯ Text-Driven Image Segmentation with SAM 2
Assignment: Q2 For a single image, perform text-prompted segmentation of a chosen object using SAM 2.

## ğŸš€ Quick Start

```python
# 1. Run installation cells
# 2. Upload your image
# 3. Enter text prompt: "cat", "person", "car", etc.
# 4. Get segmentation results!


ğŸ“‹ Overview
Upload an image and enter a text prompt. The system automatically:
ğŸ” Generates intelligent prompt points/boxes
ğŸ¨ Segments the object using SAM 2
ğŸ“Š Displays mask and overlay
ğŸ’¾ Saves results as PNG files

Example:
Input: Image of a cat + Text: "cat"
Output: Segmentation mask highlighting the cat

ğŸ› ï¸ How It Works
Pipeline:
Text Prompt â†’ Geometric Prompts â†’ SAM 2 Model â†’ Segmentation Mask

Three-Stage Process:

Stage 1: Text-to-Prompt Conversion
Generates multiple prompt strategies from your text
Creates center points, grid points, and bounding boxes
Uses heuristics for object localization

Stage 2: SAM 2 Segmentation
Tests all prompt strategies automatically
Runs SAM 2 (Hiera-Large, 879MB)
Selects best result based on confidence scores

Stage 3: Visualization & Export
Displays original image, mask, and colored overlay
Shows segmentation statistics (coverage %, quality score)
Saves files: mask_[prompt].png & overlay_[prompt].png

ğŸ’¡ Implementation Details

Prompt Strategies:
Center Point: Single center point (good for centered objects)
Grid Points: 5-point grid for distributed objects
Center Box: 60% coverage box (for larger objects)
System automatically selects strategy with highest confidence

âœ¨ Features

âœ… Fully Automatic â€“ No manual annotation needed
âœ… Multi-Strategy â€“ Tests multiple approaches automatically
âœ… Quality Scoring â€“ Picks best segmentation result
âœ… Detailed Stats â€“ Shows coverage %, confidence scores
âœ… Export Ready â€“ Saves masks and overlays as PNG
âœ… GPU Accelerated â€“ Fast inference with CUDA support

ğŸ“Š Sample Output
==========================================================
Segmentation Statistics:
Text Prompt: 'cat'
Method Used: Center Point
Quality Score: 0.987
Mask Coverage: 23.45%
Image Size: 1024x768
==========================================================

ğŸ® Usage (Colab)
Open notebook using the badge above
Enable GPU: Runtime â†’ Change runtime type â†’ GPU
Run installation cells
Upload your image
Enter text prompt ("dog", "person", "bottle")
View results â€“ mask & overlay displayed automatically
Download output PNGs
Example Text Prompts:
"person"   # Segments people
"cat"      # Segments cats
"car"      # Segments vehicles
"bottle"   # Segments bottles
"laptop"   # Segments electronics

âš ï¸ Limitations
Simplified Prompt Generation â€“ Uses geometric heuristics, not semantic understanding
No Grounding Model â€“ Text doesnâ€™t guide point placement semantically
Object Localization â€“ May miss off-center or small objects
Performance Constraints â€“ GPU required for reasonable speed (~2-5 sec/image)
No Video Support â€“ Only static image segmentation

ğŸ“ Output Files

mask_[your_prompt].png â€“ Binary segmentation mask
overlay_[your_prompt].png â€“ Original image with mask overlay

ğŸ”® Future Improvements
Integrate GroundingDINO for semantic text-to-box conversion
Add CLIPSeg for text-guided point selection
Multi-object detection & segmentation
Video segmentation with temporal propagation
Support complex queries with spatial relationships
